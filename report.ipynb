{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main objective for this project was to get a somewhat accurate estimation of a rent price in Doha, given an input of some features, such as:\n",
    "\n",
    "- `Type`: segment of the establishment - Apartment, Villa, Penthouse, etc. \n",
    "\n",
    "- `Area(sqm)`: area of establishment in square meters - preprocessed format in the web scraping script\n",
    "\n",
    "- `Bedrooms`/`Bathrooms`: # of bedrooms/bathrooms\n",
    "\n",
    "- `Location`: location of establishment - the zone/neighborhood\n",
    "\n",
    "- `Amenities`: amenities corresponding to the establishment - Balcony/Security/View at water, etc\n",
    "\n",
    "- `Furnishing`: level of furnishing in establishment - Furnished/Partly Furnished/Unfurnished\n",
    "\n",
    "The main question is, in fact, given features x1, x2 ..., can we derive some equation y= a0 + a1 * x1 + a2 * x2 ... ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have conducted countless trial-and-error tests, as previously mentioned in the presentation and the notebooks, in order to tweak the data cleaning and wrangling process and the model, to obtain the best results. We will go through a few tests when it came to determining what estimator we should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score,explained_variance_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data_ready.csv\")\n",
    "data = dataset\n",
    "x = data.drop(columns = ['Price'])\n",
    "y = data['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on train:  0.7109591543880578\n",
      "R2 on test: 0.686721532212042\n",
      "RMSE:  2894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, test_size=0.2, shuffle=True)\n",
    "\n",
    "est = LinearRegression()\n",
    "est.fit(x_train, y_train) \n",
    "y_pred = est.predict(x_test)\n",
    "\n",
    "print(\"R2 on train: \", est.score(x_train, y_train))\n",
    "print(\"R2 on test:\", est.score(x_test, y_test))\n",
    "print(\"RMSE: \", int(np.sqrt(sm.mean_squared_error(y_test, y_pred)))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for such a restrained dataset, given that most of the observations we collected are actually clustered in different hotspots - Apartments in The Pearl for example, etc...\n",
    "\n",
    "We interpret these results in the manner that roughly 70% of the variation in the values of features can be predicted correctly by the model. Nevertheless, trying more estimators might give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on train:  -2.160787973618269e+23\n",
      "R2 on test: -2.1433485516220804e+23\n",
      "RMSE:  2270972043837340\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, test_size=0.2, shuffle=True)\n",
    "\n",
    "est = SGDRegressor()\n",
    "est.fit(x_train, y_train) \n",
    "y_pred = est.predict(x_test)\n",
    "\n",
    "print(\"R2 on train: \", est.score(x_train, y_train))\n",
    "print(\"R2 on test:\", est.score(x_test, y_test))\n",
    "print(\"RMSE: \", int(np.sqrt(sm.mean_squared_error(y_test, y_pred)))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are disasterous, but it makes sense why. First and foremost, the Stochastic Gradient Regressor is scale-sensitive, so let's redo this action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on train:  0.6988120679406404\n",
      "R2 on test: 0.7207864390054183\n",
      "RMSE:  2583\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, test_size=0.2, shuffle=True)\n",
    "\n",
    "est = SGDRegressor()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "est.fit(x_train, y_train) \n",
    "y_pred = est.predict(x_test)\n",
    "\n",
    "print(\"R2 on train: \", est.score(x_train, y_train))\n",
    "print(\"R2 on test:\", est.score(x_test, y_test))\n",
    "print(\"RMSE: \", int(np.sqrt(sm.mean_squared_error(y_test, y_pred)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it! We get very similar results with the classic Linear Regression, a little bit better even when predicting on `test`, yet when tested on the actual program and inputting some features manually, as a user, our extensive tests showed that it tends to overestimate the prices a bit, while the linear regressor keeps it closer to the real ones. Moving on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the relation between the features is not actually linear, but polynomial? We can test this out by using a Polynomial Regression to see if we get better results if we model the relation between dependent and independent variables as an nth degree polynomial function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on train:  0.8326745416081861\n",
      "R2 on test: 0.6014560514145142\n",
      "RMSE:  3200\n"
     ]
    }
   ],
   "source": [
    "est = LinearRegression()\n",
    "xp = PolynomialFeatures(degree = 2).fit_transform(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(xp, y.values, test_size=0.2, shuffle=True)\n",
    "\n",
    "PolynomialFeatures(degree = 2).fit(x_train, y_train)\n",
    "\n",
    "est.fit(x_train, y_train) \n",
    "y_pred = est.predict(x_test)\n",
    "\n",
    "print(\"R2 on train: \", est.score(x_train, y_train))\n",
    "print(\"R2 on test:\", est.score(x_test, y_test))\n",
    "print(\"RMSE: \", int(np.sqrt(sm.mean_squared_error(y_test, y_pred)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to compare, much better results on train can be observed, but when it comes to predicting on test, results are much worse. Let's see potential results if we change the degree of the polynomial to 3. Will it even run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a7d639a434eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mPolynomialFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\smartlab_2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1516\u001b[0m                                           self.include_bias)\n\u001b[0;32m   1517\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_input_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_output_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\smartlab_2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1516\u001b[0m                                           self.include_bias)\n\u001b[0;32m   1517\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_input_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_output_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "est = LinearRegression()\n",
    "xp = PolynomialFeatures(degree = 3).fit_transform(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(xp, y.values, test_size=0.2, shuffle=True)\n",
    "\n",
    "PolynomialFeatures(degree = 3).fit(x_train, y_train)\n",
    "\n",
    "est.fit(x_train, y_train) \n",
    "y_pred = est.predict(x_test)\n",
    "\n",
    "print(\"R2 on train: \", est.score(x_train, y_train))\n",
    "print(\"R2 on test:\", est.score(x_test, y_test))\n",
    "print(\"RMSE: \", int(np.sqrt(sm.mean_squared_error(y_test, y_pred)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it didn't! Too much time has passed and it's still going, we are using way too many features to be able to test this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Future Prospects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is safe to say that some features are of greater influence than others. Even logically speaking, the number of bedrooms or the level of furnishment within the establishment will clearly have a greater impact on the price than having a \"Walk-in closet\" or a \"Concierge\" as some of the amenities, and as a matter of fact, we successfully achieve to predict these variations with our model. To be more precise, in terms of numbers, our Linear Regression estimator scores a very stable 70% \"accuracy\" in predicting those variations, after we tweaked and engineered all the features in so many variations and combinations, which is the reason why we chose to approach this numerical estimation problem using a classic linear regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, although 70% \"accuracy\" does not sound too bad, it is critical to mention here the impact of the human factor in this entire scenario. While more or less predictable, these listings come from different agencies, with different commisions, with different amenities which we were not able to collect in a straightforward manner, such as utilities included, or \"2 months free\" (and prices are usually higher in this case, although in theory they should not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some very important points that would help improve the model in the future:\n",
    "\n",
    "- More close and in-detail review of the importance and magnitude applied by certain features to achieve better results\n",
    "- Description scripting using NLP to collect some new features like \"utilities included\", \"2 months free\", something that would help deal with the human factor a little bit more in-depth\n",
    "- Testing more options for feature encoding combinations, although whatever we have achieved now has come to great lengths in terms of performance, there can always be some aspects to improve\n",
    "- Last but not least, more observations. We would need way more data than whatever we collected in order to obtain more accurate estimations. This can be done by obtaining some databases from different rent advertisement providers, or obtaining more historical data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
